{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "381f3144",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchvision in c:\\users\\liuq\\appdata\\roaming\\python\\python39\\site-packages (0.15.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: torch==2.0.0 in c:\\users\\liuq\\appdata\\roaming\\python\\python39\\site-packages (from torchvision) (2.0.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchvision) (2.8.4)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchvision) (1.11.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchvision) (4.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchvision) (2.11.3)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchvision) (3.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (2022.9.24)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch==2.0.0->torchvision) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch==2.0.0->torchvision) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bf98f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python imports\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# helper functions for computer vision\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets\n",
    "\n",
    "import argparse\n",
    "\n",
    "import scipy.io\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b8403a6-9f31-4d04-a2c3-9ab7296fbcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MNIST\n",
    "\n",
    "from torchvision.datasets import EMNIST\n",
    "#torchvision.datasets.EMNIST()\n",
    "# Download and Save MNIST \n",
    "digits_train = EMNIST( './emnist_data', train=True, split=\"mnist\", download=True, transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                                                                                                  #transforms.RandomCrop(size = 32, padding = 2, fill=0), \n",
    "                                                                                                                  lambda img: torchvision.transforms.functional.rotate(img, -90),\n",
    "                                                                                                                  lambda img: torchvision.transforms.functional.hflip(img)]))\n",
    "digits_train_large = EMNIST( './emnist_data', train=True, split=\"digits\", download=True, transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                                                                                                         #transforms.RandomCrop(size = 32, padding = 2, fill=0),\n",
    "                                                                                                                         lambda img: torchvision.transforms.functional.rotate(img, -90),\n",
    "                                                                                                                         lambda img: torchvision.transforms.functional.hflip(img)]))\n",
    "letters_train = EMNIST( './emnist_data', train=True, split=\"letters\", download=True, transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                                                                                                     #transforms.RandomCrop(size = 32, padding = 2, fill=0),\n",
    "                                                                                                                     lambda img: torchvision.transforms.functional.rotate(img, -90),\n",
    "                                                                                                                     lambda img: torchvision.transforms.functional.hflip(img)]))\n",
    "letters_train.targets += 9\n",
    "\n",
    "digits_test = EMNIST( './emnist_data', train=False, split=\"mnist\", download=True, transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                                                                                                  #transforms.RandomCrop(size = 32, padding = 2, fill=0), \n",
    "                                                                                                                  lambda img: torchvision.transforms.functional.rotate(img, -90),\n",
    "                                                                                                                  lambda img: torchvision.transforms.functional.hflip(img)]))\n",
    "digits_test_large = EMNIST( './emnist_data', train=False, split=\"digits\", download=True, transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                                                                                                         #transforms.RandomCrop(size = 32, padding = 2, fill=0),\n",
    "                                                                                                                         lambda img: torchvision.transforms.functional.rotate(img, -90),\n",
    "                                                                                                                         lambda img: torchvision.transforms.functional.hflip(img)]))\n",
    "letters_test = EMNIST( './emnist_data', train=False, split=\"letters\", download=True, transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                                                                                                     #transforms.RandomCrop(size = 32, padding = 2, fill=0),\n",
    "                                                                                                                     lambda img: torchvision.transforms.functional.rotate(img, -90),\n",
    "                                                                                                                     lambda img: torchvision.transforms.functional.hflip(img)]))\n",
    "letters_test.targets += 9\n",
    "\n",
    "#comb_train = torch.utils.data.ConcatDataset([digits_train,letters_train])\n",
    "comb_train = digits_train_large\n",
    "#comb_test = torch.utils.data.ConcatDataset([digits_test,letters_test])\n",
    "comb_test = digits_test_large\n",
    "#data_train = torch.utils.data.DataLoader(comb_train , batch_size=128,shuffle=True )\n",
    "\n",
    "#data_test = torch.utils.data.DataLoader( comb_test , batch_size=128,shuffle=True )\n",
    "#for batch_idx, samples in enumerate(data_train):\n",
    "    #print(batch_idx, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12cc93f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_train[5][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be603b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(comb_train)):\n",
    "#     temp = list(comb_train[i])\n",
    "#     temp_matrix = temp[0][0].clone().T\n",
    "#     temp[0][0] = temp_matrix\n",
    "#     comb_train[i] = tuple(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a32dd70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1fd3d6f82e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize=(1, 1))\n",
    "cols, rows = 1, 1\n",
    "sample_idx = 5\n",
    "img, label = comb_test[sample_idx]\n",
    "figure.add_subplot(rows, cols, 1)\n",
    "plt.title(label) \n",
    "plt.axis(\"off\")\n",
    "plt.imshow(img.view(28,28), cmap=\"gray\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80377fec-fabb-4a1f-b028-3b0b4f7921a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[32, 32]' is invalid for input of size 784",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\Temp\\ipykernel_3376\\4146959555.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"off\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"gray\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;31m#plt.show()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[32, 32]' is invalid for input of size 784"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMoAAADdCAYAAAD6tu2LAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAADBklEQVR4nO3cIW4bURhG0X+qovDgeDUBRl5NcNYRZQnm2UloFB7LwAoIckGrqqqq6KKOq55Dh3zkSvMeeMv5fD4P8Kkvaw+Af4FQIBAKBEKBQCgQCAUCoUAgFAiEAoFQLsTpdJq7u7u5vb2d6+vrWZZl7u/v157FD0K5EIfDYR4eHubj42N2u93ac/jN17UH8N3Nzc0cj8dZlmXe3t7m8fFx7Un8QigXYlmWtSfwCb9eEAgFAqFAIBQIhAKBUCBwPXxBnp6e5v39fU6n08zMPD8/z36/n5mZ7XY7V1dXa877ry0el7gcm81mXl9f//jt5eVlNpvN3x3ET0KBwBkFAqFAIBQIhAKBUCAQCgRCgUAoEAgFAqFAIBQIhAKBUCAQCgRCgUAoEAgFAqFAIBQIhAKBUCAQCgRCgUAoEAgFAqFAIBQIhAKBUCAQCgRCgUAoEAgFAqFAIBQIhAKBUCAQCgRCgUAoEAgFAqFAIBQIhAKBUCAQCgRCgUAoEAgFAqFAIBQIhAKBUCAQCgRCgUAoEAgFAqFAIBQIhAKBUCAQCgRCgUAoEAgFAqFAIBQIhAKBUCAQCgRCgUAoEAgFAqFAIBQIhAKBUCAQCgRCgUAoEAgFAqFAIBQIhAKBUCAQCgRCgUAoEAgFAqFAIBQIhAKBUCAQCgRCgUAoEAgFAqFAIBQIhAKBUCAQCgRCgUAoEAgFAqFAIBQIhAKBUCAQCgRCgUAoEAgFAqFAIBQIhAKBUCAQCgRCgUAoEAgFAqFAIBQIhAKBUCAQCgRCgUAoEAgFAqFAIBQIhAKBUCAQCgRCgUAoEAgFAqFAIBQIhAKBUCAQCgRCgUAoEAgFAqFAIBQIhAKBUCAQCgRCgUAoEAgFAqFAIBQIhAKBUCAQCgRCgUAoEAgFAqFAIBQIhAKBUCAQCgRCgUAoEAgFAqFAIBQIhAKBUCAQCgRCgUAoEAgFAqFAIBQIhAKBUCAQCgRCgUAoEAgFAqFAIBQIhAKBUCAQCgRCgUAoEAgFAqFAIBQIhAKBUCAQCgTfAIQ3OBgkhYHrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(comb_test), size=(1,)).item()\n",
    "    img, label = comb_test[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label) \n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.view(32,32), cmap=\"gray\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7022915",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, input_shape=(28, 28), num_classes=10):\n",
    "        super(LeNet, self).__init__()\n",
    "        # certain definitions\n",
    "        self.num_classes = num_classes\n",
    "        self.input_shape = input_shape\n",
    "        self.conv1 = nn.Conv2d(1,6,5)\n",
    "        self.conv2 = nn.Conv2d(6,16,5)\n",
    "        #self.lin1 = nn.Linear(400,120)\n",
    "        self.lin1 = nn.Linear(784,64)\n",
    "        #self.lin2 = nn.Linear(120,84)\n",
    "        self.lin2 = nn.Linear(64,num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        shape_dict = {}\n",
    "        \n",
    "        # neuron network\n",
    "        #Layer3\n",
    "        x = torch.flatten(x,1)\n",
    "        shape_dict[1] = list(x.size())\n",
    "        #Layer4\n",
    "        x = F.relu(self.lin1(x))\n",
    "        shape_dict[2] = list(x.size())\n",
    "        #Layer5\n",
    "        # x = F.relu(self.lin2(x))\n",
    "        # shape_dict[3] = list(x.size())\n",
    "        #Layer6\n",
    "        out = self.lin2(x)\n",
    "        shape_dict[3] = list(out.size())\n",
    "        # out = self.lin3(x)\n",
    "        # shape_dict[4] = list(out.size())\n",
    "        '''\n",
    "        # CNN\n",
    "        #Layer1\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)),kernel_size=2,stride=2)\n",
    "        shape_dict[1] = list(x.size())\n",
    "        #Layer2\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)),kernel_size=2,stride=2)\n",
    "        shape_dict[2] = list(x.size())\n",
    "        #Layer3\n",
    "        x = torch.flatten(x,1)\n",
    "        shape_dict[3] = list(x.size())\n",
    "        #Layer4\n",
    "        x = F.relu(self.lin1(x))\n",
    "        shape_dict[4] = list(x.size())\n",
    "        #Layer5\n",
    "        x = F.relu(self.lin2(x))\n",
    "        shape_dict[5] = list(x.size())\n",
    "        #Layer6\n",
    "        out = self.lin3(x)\n",
    "        shape_dict[6] = list(out.size())\n",
    "        '''\n",
    "\n",
    "        return out, shape_dict\n",
    "\n",
    "\n",
    "def count_model_params():\n",
    "    '''\n",
    "    return the number of trainable parameters of LeNet.\n",
    "    '''\n",
    "    model = LeNet()\n",
    "    model_params = 0.0\n",
    "    for name, param in model.named_parameters():\n",
    "        temp_result = 1\n",
    "        for each in param.size():\n",
    "            temp_result = temp_result*each\n",
    "        model_params += temp_result\n",
    "    return model_params/1000000\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, optimizer, criterion, epoch):\n",
    "    \"\"\"\n",
    "    model (torch.nn.module): The model created to train\n",
    "    train_loader (pytorch data loader): Training data loader\n",
    "    optimizer (optimizer.*): A instance of some sort of optimizer, usually SGD\n",
    "    criterion (nn.CrossEntropyLoss) : Loss function used to train the network\n",
    "    epoch (int): Current epoch number\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for input, target in tqdm(train_loader, total=len(train_loader)):\n",
    "        ###################################\n",
    "        # fill in the standard training loop of forward pass,\n",
    "        # backward pass, loss computation and optimizer step\n",
    "        ###################################\n",
    "        #might want to fix later TODO\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # 1) zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # 2) forward + backward + optimize\n",
    "        output, _ = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the train_loss variable\n",
    "        # .item() detaches the node from the computational graph\n",
    "        # Uncomment the below line after you fill block 1 and 2\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    print('[Training set] Epoch: {:d}, Average loss: {:.4f}'.format(epoch+1, train_loss))\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def test_model(model, test_loader, epoch):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for input, target in test_loader:\n",
    "            input = input.to(device)\n",
    "            target = target.to(device)\n",
    "            output, _ = model(input)\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_acc = correct / len(test_loader.dataset)\n",
    "    print('[Test set] Epoch: {:d}, Accuracy: {:.2f}%\\n'.format(\n",
    "        epoch+1, 100. * test_acc))\n",
    "\n",
    "    return test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e94d901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best,\n",
    "                    file_folder=\"./outputs/\",\n",
    "                    filename='checkpoint.pth.tar'):\n",
    "    \"\"\"save checkpoint\"\"\"\n",
    "    if not os.path.exists(file_folder):\n",
    "        os.makedirs(os.path.expanduser(file_folder), exist_ok=True)\n",
    "    torch.save(state, os.path.join(file_folder, filename))\n",
    "    if is_best:\n",
    "        # skip the optimization state\n",
    "        state.pop('optimizer', None)\n",
    "        torch.save(state, os.path.join(file_folder, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff9b5d5f-7b1d-47d7-9854-f182c5893d43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:25<00:00, 22.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training set] Epoch: 1, Average loss: 0.5543\n",
      "[Test set] Epoch: 1, Accuracy: 92.15%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:24<00:00, 22.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training set] Epoch: 2, Average loss: 0.2545\n",
      "[Test set] Epoch: 2, Accuracy: 93.51%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:25<00:00, 21.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training set] Epoch: 3, Average loss: 0.2170\n",
      "[Test set] Epoch: 3, Accuracy: 94.31%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:25<00:00, 22.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training set] Epoch: 4, Average loss: 0.1918\n",
      "[Test set] Epoch: 4, Accuracy: 94.90%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:24<00:00, 22.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training set] Epoch: 5, Average loss: 0.1719\n",
      "[Test set] Epoch: 5, Accuracy: 95.39%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:26<00:00, 21.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training set] Epoch: 6, Average loss: 0.1560\n",
      "[Test set] Epoch: 6, Accuracy: 95.77%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:25<00:00, 21.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training set] Epoch: 7, Average loss: 0.1431\n",
      "[Test set] Epoch: 7, Accuracy: 96.10%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:27<00:00, 21.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training set] Epoch: 8, Average loss: 0.1325\n",
      "[Test set] Epoch: 8, Accuracy: 96.37%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:25<00:00, 21.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training set] Epoch: 9, Average loss: 0.1236\n",
      "[Test set] Epoch: 9, Accuracy: 96.59%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:25<00:00, 21.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training set] Epoch: 10, Average loss: 0.1162\n",
      "[Test set] Epoch: 10, Accuracy: 96.76%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:25<00:00, 21.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training set] Epoch: 11, Average loss: 0.1098\n",
      "[Test set] Epoch: 11, Accuracy: 96.95%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:25<00:00, 21.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training set] Epoch: 12, Average loss: 0.1044\n",
      "[Test set] Epoch: 12, Accuracy: 97.05%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:25<00:00, 21.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training set] Epoch: 13, Average loss: 0.0995\n",
      "[Test set] Epoch: 13, Accuracy: 97.16%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:25<00:00, 21.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training set] Epoch: 14, Average loss: 0.0953\n",
      "[Test set] Epoch: 14, Accuracy: 97.29%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:26<00:00, 21.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training set] Epoch: 15, Average loss: 0.0914\n",
      "[Test set] Epoch: 15, Accuracy: 97.39%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:26<00:00, 21.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training set] Epoch: 16, Average loss: 0.0879\n",
      "[Test set] Epoch: 16, Accuracy: 97.45%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:25<00:00, 21.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training set] Epoch: 17, Average loss: 0.0849\n",
      "[Test set] Epoch: 17, Accuracy: 97.58%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:26<00:00, 21.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training set] Epoch: 18, Average loss: 0.0821\n",
      "[Test set] Epoch: 18, Accuracy: 97.60%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:26<00:00, 21.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training set] Epoch: 19, Average loss: 0.0795\n",
      "[Test set] Epoch: 19, Accuracy: 97.66%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1875/1875 [01:25<00:00, 21.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training set] Epoch: 20, Average loss: 0.0771\n",
      "[Test set] Epoch: 20, Accuracy: 97.74%\n",
      "\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "class MyArgs:\n",
    "    resume = False #\"./outputs/checkpoint.pth.tar\"\n",
    "    epochs = 20\n",
    "    lr = 0.002\n",
    "    batch_size = 128\n",
    "args = MyArgs()\n",
    "\n",
    "# main function for training and testing\n",
    "\n",
    "# set up random seed\n",
    "#torch.manual_seed(0)\n",
    "\n",
    "###################################\n",
    "# setup model, loss and optimizer #\n",
    "###################################\n",
    "\n",
    "model = LeNet()\n",
    "\n",
    "training_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\n",
    "# optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "# set up transforms to transform the PIL Image to tensors\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "################################\n",
    "# setup dataset and dataloader #\n",
    "################################\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(comb_train , batch_size=args.batch_size,shuffle=True )\n",
    "test_loader = torch.utils.data.DataLoader( comb_test , batch_size=args.batch_size,shuffle=True )\n",
    "\n",
    "################################\n",
    "# start the training           #\n",
    "################################\n",
    "# resume from a previous checkpoint\n",
    "best_acc = 0.0\n",
    "start_epoch = 0\n",
    "if args.resume:\n",
    "    if os.path.isfile(args.resume):\n",
    "        print(\"=> loading checkpoint '{:s}'\".format(args.resume))\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_acc = checkpoint['best_acc']\n",
    "        # load model weight\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        # load optimizer states\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(\"=> loaded checkpoint '{:s}' (epoch {:d}, acc {:0.2f})\".format(\n",
    "            args.resume, checkpoint['epoch'], 100*best_acc))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "\n",
    "# training of the model\n",
    "print(\"Training the model ...\\n\")\n",
    "for epoch in range(start_epoch, args.epochs):\n",
    "    # train model for 1 epoch\n",
    "    train_model(model, train_loader, optimizer, training_criterion, epoch)\n",
    "    # evaluate the model on test_set after this epoch\n",
    "    acc = test_model(model, test_loader, epoch)\n",
    "    # save the current checkpoint\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_acc' : max(best_acc, acc),\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "        }, (acc > best_acc))\n",
    "    best_acc = max(best_acc, acc)\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2146856e-1a7d-4b67-aca8-914dbe116cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (lin1): Linear(in_features=784, out_features=64, bias=True)\n",
       "  (lin2): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f0cef94-fd05-46ff-9b0c-e10725f9ed1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53462.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_model_params() *1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "997e7b0e-be0a-4a20-9ea1-268239b10217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0270, -0.0192, -0.0298,  ...,  0.0139,  0.0238,  0.0304],\n",
       "        [-0.0343, -0.0235, -0.0044,  ..., -0.0180,  0.0076, -0.0225],\n",
       "        [-0.0253, -0.0183, -0.0170,  ..., -0.0110, -0.0261,  0.0328],\n",
       "        ...,\n",
       "        [ 0.0329, -0.0170, -0.0136,  ...,  0.0247, -0.0033,  0.0128],\n",
       "        [ 0.0084,  0.0289, -0.0187,  ...,  0.0316, -0.0141, -0.0178],\n",
       "        [-0.0255,  0.0194,  0.0118,  ...,  0.0182,  0.0015,  0.0234]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lin1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cbd8705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50176"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.lin1.weight.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38367cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "write_file = open('weightfix.hex', 'w')\n",
    "for i in range(len(model.lin1.weight.reshape(-1))):\n",
    "    s_print = \"@\"+hex(i)[2:].zfill(4)+\" \"+hex(struct.unpack('<I', struct.pack('<f', model.lin1.weight.reshape(-1)[i].item()))[0])[2:].zfill(8)\n",
    "    write_file.write(s_print.strip()+'\\n')\n",
    "write_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c71f9e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@c400 3d82e19b|@c401 3e3c6cee|@c402 bf07cef1|@c403 3de77e8d|@c404 3ee55647|@c405 be75f5a7|@c406 bd9cf0ec|@c407 3e1cd669|@c408 3ed2c6b3|@c409 be96a05c|@c40a 3e9b1cea|@c40b 3e6292e5|@c40c be541d53|@c40d 3d7186d6|@c40e bd99809f|@c40f bd789cdd|@c410 3eab56ee|@c411 bea3d471|@c412 3da75720|@c413 3dd5efb2|@c414 3e9981f0|@c415 3e5c2bd7|@c416 be03fa0e|@c417 bd429d9d|@c418 bed093db|@c419 bf147746|@c41a 3ef1bc48|@c41b 3edec1ad|@c41c be004451|@c41d 3f26cbdb|@c41e 3d94cb08|@c41f be76bb90|@c420 3e8d4f16|@c421 3e3ec2d8|@c422 bf14ac4c|@c423 3d849222|@c424 be59723a|@c425 bdf9be3c|@c426 3ee2ef04|@c427 be362856|@c428 3e4eace0|@c429 bedce75f|@c42a bf0d4cde|@c42b bd47a1f7|@c42c 3e8e72c8|@c42d beb6f88b|@c42e befcf4e8|@c42f bed65f1a|@c430 be468093|@c431 3eba1929|@c432 bc8d2bce|@c433 bf18eaf0|@c434 3d8d9c2c|@c435 3d13c662|@c436 3eb3c91b|@c437 bf261ae6|@c438 3eb7fce7|@c439 3c24d928|@c43a be65aaf7|@c43b bebbda6d|@c43c be358772|@c43d 3c71e717|@c43e be5d3139|@c43f be876ab4|@c440 3e9f493f|@c441 be1bc127|@c442 be528592|@c443 3e63e07f|@c444 bea8ee1b|@c445 3f3fc0b8|@c446 3f29e858|@c447 bdfc3c0b|@c448 bc5156ee|@c449 3f0260ac|@c44a bec70535|@c44b 3e251004|@c44c 3e12bb6b|@c44d 3e547cf9|@c44e bc1d66e6|@c44f 3ecdb907|@c450 3e4eae79|@c451 3ed430b9|@c452 3e948110|@c453 bee15d02|@c454 be05eeb1|@c455 bdb68e03|@c456 3e215b29|@c457 be3b191d|@c458 be8bbbbf|@c459 3c933141|@c45a be8311ab|@c45b 3f14f7a4|@c45c bf1caafc|@c45d 3e8bad00|@c45e 3dfbae56|@c45f 3f0bd934|@c460 3ea83d3f|@c461 becee2df|@c462 3f1d32f9|@c463 3e15fc89|@c464 3b51b297|@c465 bf000e15|@c466 bef89f33|@c467 be9a353b|@c468 bcce1ea8|@c469 bef5d8bb|@c46a 3f46738c|@c46b bd0c7ea3|@c46c be4d86a0|@c46d 3e458c15|@c46e 3ea0d028|@c46f be7cf860|@c470 3ee9df97|@c471 3da99928|@c472 3f54e215|@c473 be41db7e|@c474 bdd621c3|@c475 bcf9f42e|@c476 be84737b|@c477 3df28fa5|@c478 bf209b54|@c479 beab5a27|@c47a be9b0156|@c47b 3ec1d498|@c47c 3f077c7d|@c47d bda89f7d|@c47e bd58d354|@c47f be97386f|@c480 3f01e6cd|@c481 3e1a18b8|@c482 3f44e525|@c483 bf5d2fe4|@c484 bdea7d8f|@c485 be23f130|@c486 be3eb2f8|@c487 3e390e46|@c488 3ea1aed9|@c489 3ed251e6|@c48a 3ed23c08|@c48b bd32ebb7|@c48c 3e4c0591|@c48d bc72494e|@c48e 3daf77bf|@c48f 3f1eff53|@c490 3e6bc025|@c491 bea8eed4|@c492 3df7c4a7|@c493 3e834c1c|@c494 3ead711a|@c495 3ee5a4f1|@c496 3e94b28a|@c497 3ebc0c79|@c498 3edc3a27|@c499 3e9824c8|@c49a be6e968b|@c49b 3e8ac1ab|@c49c bee34326|@c49d bf0bc2ea|@c49e be15b975|@c49f be956844|@c4a0 3da00b88|@c4a1 bdafbf04|@c4a2 3ea2bfde|@c4a3 3e371757|@c4a4 be9ddad7|@c4a5 3f42fc54|@c4a6 3eac03c6|@c4a7 3edf4597|@c4a8 be4b396f|@c4a9 bf0a2365|@c4aa 3d802972|@c4ab bef24fa5|@c4ac be3ad83e|@c4ad bf1074ba|@c4ae becc6826|@c4af be947920|@c4b0 3f093bd2|@c4b1 be36dbeb|@c4b2 be87c42c|@c4b3 3e3ee343|@c4b4 bec19cef|@c4b5 be877b17|@c4b6 bea3139f|@c4b7 be5b33f4|@c4b8 bdafb898|@c4b9 3e32f08c|@c4ba bd322595|@c4bb 3e7d9252|@c4bc 3e7da90b|@c4bd bd597639|@c4be 3e592131|@c4bf bd216399|@c4c0 bea21999|@c4c1 bd2d6668|@c4c2 be51a4d7|@c4c3 be6aacc8|@c4c4 bec547aa|@c4c5 bda066c4|@c4c6 3eb0989e|@c4c7 3e3588a0|@c4c8 bde10b95|@c4c9 3e3c5321|@c4ca 3e5f246c|@c4cb bd861446|@c4cc 3e9812be|@c4cd 3dbff848|@c4ce bd0402e4|@c4cf be8d80ee|@c4d0 be275a62|@c4d1 bf4d4fd0|@c4d2 bdda5b9e|@c4d3 be871a7b|@c4d4 3cd8e37e|@c4d5 3e3c900d|@c4d6 3e343455|@c4d7 3ec446dd|@c4d8 bc371917|@c4d9 be4b3bdb|@c4da be76efe4|@c4db 3f04c865|@c4dc 3e849b54|@c4dd bf2e3efc|@c4de be23337a|@c4df badc3b47|@c4e0 3eb9694a|@c4e1 3e9d098d|@c4e2 bea1ca10|@c4e3 be0c138a|@c4e4 3e21e44c|@c4e5 3ec099a6|@c4e6 3e76d7a9|@c4e7 3e9c8848|@c4e8 be97d276|@c4e9 3e8b9b02|@c4ea bf502586|@c4eb 3e35a43c|@c4ec 3e86dbd3|@c4ed beaf27ad|@c4ee 3ee33471|@c4ef 3e0729ff|@c4f0 3e322ef7|@c4f1 bf2701c3|@c4f2 be233ffd|@c4f3 bcccdb72|@c4f4 bea6d841|@c4f5 3e305403|@c4f6 be0e1a51|@c4f7 3f43409a|@c4f8 3e8d6c07|@c4f9 3e4c41ac|@c4fa bf6894c7|@c4fb bea1ae34|@c4fc 3ec54813|@c4fd 3e3dc7ef|@c4fe be4af6c2|@c4ff 38b4b5ad|@c500 3e98a590|@c501 bdf7621b|@c502 3f5f9aa3|@c503 3ec1adeb|@c504 3d953225|@c505 3d8fd144|@c506 3e5a4a26|@c507 be62ce07|@c508 3e01bb29|@c509 bda6923b|@c50a be9e063e|@c50b 3db745bd|@c50c bf689174|@c50d 3ebf9c49|@c50e 3e0857ae|@c50f be1a6937|@c510 be895f84|@c511 3d8e6b36|@c512 bf304747|@c513 3da29a7d|@c514 be8b6b7f|@c515 3e67ae36|@c516 bba594c2|@c517 bf3e3f21|@c518 3d1a8cc1|@c519 3e817092|@c51a bca1fa42|@c51b bf663803|@c51c 3dec90f5|@c51d be42c464|@c51e 3f4eadad|@c51f bd5b6eb3|@c520 bea78893|@c521 3ecdaa14|@c522 bf0af7cd|@c523 3dace11e|@c524 3db08eac|@c525 bf5338ef|@c526 bf24d1f9|@c527 be291a4a|@c528 3f2432ea|@c529 3f0813d6|@c52a 3ea0706a|@c52b 3e31cd71|@c52c beea2f73|@c52d bef5311a|@c52e be616c10|@c52f 3f0f57a2|@c530 be5c9812|@c531 3ef59de5|@c532 3f53bb2b|@c533 3e7a4d4d|@c534 3e822bc4|@c535 3d270956|@c536 bea98bda|@c537 3dc04ca4|@c538 bdcbc49c|@c539 be8db530|@c53a 3d8f0378|@c53b bd3349b9|@c53c be834f1e|@c53d be6214b7|@c53e 3e08d6d7|@c53f 3eb8fb60|@c540 bf5f3ed1|@c541 3ec6a029|@c542 beb109e8|@c543 3e6f499c|@c544 3d0de8cf|@c545 3ed9dba8|@c546 bf5f7bb6|@c547 3d512cee|@c548 bdab9972|@c549 be285df3|@c54a be4933ff|@c54b 3c71e935|@c54c 3ef83647|@c54d be959e38|@c54e 3cba5675|@c54f 3dcfad15|@c550 3ef4e1bf|@c551 3e425aad|@c552 3f2e88f1|@c553 3ddd1157|@c554 bec9175f|@c555 beabc2e8|@c556 3ec0a7d5|@c557 be181e82|@c558 beeb7d6f|@c559 3ea6751a|@c55a 3e528ef7|@c55b bea596bc|@c55c 3f00d164|@c55d 3ea2dba2|@c55e be8141a3|@c55f 3cfc0a70|@c560 3f30b2b2|@c561 be2fc973|@c562 be4db18c|@c563 be143d55|@c564 3f5c379f|@c565 3ed453b1|@c566 be516e32|@c567 3f004718|@c568 be0f4fc7|@c569 be008e50|@c56a bf0c7550|@c56b 3e86cbd0|@c56c 3ecd86f3|@c56d 3f6693ed|@c56e 3f464b14|@c56f beaf1d82|@c570 bef4b151|@c571 bf138971|@c572 bf0c1cd8|@c573 3f0fcefa|@c574 3df2a5af|@c575 be86d178|@c576 3eb8f19c|@c577 3f13c91f|@c578 be4dfff6|@c579 be490e1f|@c57a 3f215d65|@c57b 3e34baa0|@c57c be621bd0|@c57d 3f71afc9|@c57e bcb8dfd1|@c57f 3f090a7a|@c580 bdd455c0|@c581 bebb96ec|@c582 3d0d378e|@c583 3bfc0374|@c584 3f06dad4|@c585 be3e0f80|@c586 be72ec77|@c587 be6fefd7|@c588 3e0881ca|@c589 3ee2fc3e|@c58a beb8e7a5|@c58b 3e75fd59|@c58c be690c41|@c58d bd6a9356|@c58e 3df0167d|@c58f beb8f805|@c590 be4fce51|@c591 3e69e26f|@c592 be96b1de|@c593 3d962eac|@c594 3e4f4f45|@c595 3e94b899|@c596 3e4b7f7e|@c597 be802b4c|@c598 3cf6cb6d|@c599 bd3a830d|@c59a 3e3fecd0|@c59b bf493c71|@c59c 3bb0a31a|@c59d 3ea64330|@c59e 3daa93e1|@c59f 3e974a35|@c5a0 bd64ce83|@c5a1 be65f72f|@c5a2 bebea548|@c5a3 3d7e36c6|@c5a4 be2ea51a|@c5a5 becc6a85|@c5a6 bf1ed47d|@c5a7 beaf0732|@c5a8 3eaae333|@c5a9 bea64533|@c5aa 3e6f9f1c|@c5ab bedc8895|@c5ac 3d86fbfb|@c5ad 3f1747aa|@c5ae 3e27eac4|@c5af bed7342e|@c5b0 be0dbdb8|@c5b1 be8db58b|@c5b2 beb47846|@c5b3 3f136ec2|@c5b4 3f34b666|@c5b5 beac5b33|@c5b6 3d37f50b|@c5b7 be4f7143|@c5b8 3ebc9590|@c5b9 be83309b|@c5ba 3ee7d874|@c5bb bc88065b|@c5bc 3e31bb51|@c5bd beb6e82e|@c5be 3e38eafe|@c5bf bf1f33f5|@c5c0 3e87de6a|@c5c1 bed66164|@c5c2 be9ff481|@c5c3 be3b0c1a|@c5c4 bf3c2523|@c5c5 3dbd26df|@c5c6 3e17d8d4|@c5c7 3dbfdcb3|@c5c8 3dc9a039|@c5c9 3de674ef|@c5ca 3f050fec|@c5cb 3ee324a0|@c5cc 3ea0197f|@c5cd 3efad6d2|@c5ce 3d83b314|@c5cf 3dcf45d3|@c5d0 3f14345e|@c5d1 3e2a73cd|@c5d2 3e41fbf9|@c5d3 bc009266|@c5d4 be9abc99|@c5d5 be2c6d4e|@c5d6 3ee20d87|@c5d7 3f3015ee|@c5d8 bd1917f3|@c5d9 becdcefc|@c5da be602666|@c5db 3eea6357|@c5dc bcf81fed|@c5dd 3d97b576|@c5de 3e367498|@c5df be36c852|@c5e0 3bd043dc|@c5e1 bd8e2ca1|@c5e2 3f3f1f55|@c5e3 3cb99ab1|@c5e4 bd967ef4|@c5e5 be3bf743|@c5e6 3eeec815|@c5e7 3e742776|@c5e8 3e12ab9a|@c5e9 3e861621|@c5ea 3edb77cd|@c5eb 3ef4f82d|@c5ec bf0c5b74|@c5ed be539523|@c5ee bf8b22cd|@c5ef 3e9583dc|@c5f0 3db02e20|@c5f1 3ea350ee|@c5f2 3ec7ec17|@c5f3 bd15c419|@c5f4 bf099609|@c5f5 bcf52608|@c5f6 bcf384cd|@c5f7 bd9d3ec1|@c5f8 3cb8465b|@c5f9 3dcda0d6|@c5fa 3e70ae0f|@c5fb bed6a726|@c5fc bf5d0cf0|@c5fd 3ef325c4|@c5fe bea7d56e|@c5ff be302de0|@c600 bd026823|@c601 3f054207|@c602 bf1cf2b3|@c603 3da9fac9|@c604 3f09eeeb|@c605 3d2bc99a|@c606 bef5390f|@c607 bde97ddf|@c608 bf381a37|@c609 bd9ef7c9|@c60a be34fefe|@c60b bf3c476a|@c60c 3e127d09|@c60d be120d82|@c60e 3dae9bc9|@c60f 3dd1ace5|@c610 bf1983cd|@c611 be49e02c|@c612 3bdd13a4|@c613 3ea32e26|@c614 3ccd6db0|@c615 bf047c5f|@c616 befc23c6|@c617 3de3394b|@c618 3ed213a6|@c619 3ed02409|@c61a 3e1fb573|@c61b bea54f4d|@c61c 3d6ee3e2|@c61d be186201|@c61e bf53640e|@c61f be7c1033|@c620 bf82fe54|@c621 be2061c7|@c622 bdb4699e|@c623 bd9652c0|@c624 be2cdfa8|@c625 3f0956a3|@c626 3dab4047|@c627 bf05ebab|@c628 bf8b577d|@c629 3ea3b6a4|@c62a 3e4f9762|@c62b be6eedf7|@c62c b9db1bc1|@c62d bcbf78d7|@c62e 3e777996|@c62f 3e971ecb|@c630 be904b85|@c631 3d96d059|@c632 bf2365ab|@c633 3e6034a3|@c634 bdc56e34|@c635 3eb1d7f4|@c636 bd774d7a|@c637 bf2d2cda|@c638 3f0b6d2e|@c639 3e8e888c|@c63a be21813e|@c63b bdc344ae|@c63c 3ea11e30|@c63d be934a70|@c63e 3e311193|@c63f 3df191e8|@c640 bda1552f|@c641 bdea7ccf|@c642 3e91fec1|@c643 3ea78f4d|@c644 bdf8dd56|@c645 bf148d62|@c646 3edd1b43|@c647 3cd2d013|@c648 beba296c|@c649 bf95c921|@c64a 3e7702bb|@c64b 3dae8da6|@c64c 3e81f93a|@c64d bf25a816|@c64e 3db59181|@c64f be0cfc73|@c650 bf24b811|@c651 3ea40251|@c652 bde653d3|@c653 bdd10c90|@c654 3e7e6b77|@c655 beaf3403|@c656 bf564775|@c657 be0d54f8|@c658 3ef3ba38|@c659 bddef047|@c65a bd87e7bc|@c65b bdefad3e|@c65c 3eaef1e1|@c65d 3dce7690|@c65e 3e0fb18b|@c65f 3e81b0c8|@c660 be8a401a|@c661 bb2733ae|@c662 3c73b028|@c663 be9466dc|@c664 be48ace4|@c665 be2a16cf|@c666 3ede100f|@c667 bc83eceb|@c668 3eacfc65|@c669 3ed5d343|@c66a be76b22a|@c66b 3e0a22b9|@c66c 3ea487b8|@c66d 3da1d3d6|@c66e bd3180ba|@c66f 3e8f9a0d|@c670 3e213986|@c671 3e57876a|@c672 be93ddde|@c673 bf58b167|@c674 3eb5f2b2|@c675 3f242d54|@c676 3e5eb86a|@c677 3ef123d1|@c678 bef18565|@c679 3ef1cd85|@c67a 3e3267f9|@c67b 3ed3fb48|@c67c be83c0a8|@c67d be9fec74|@c67e 3e1c9757|@c67f 3ee8a138|\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(model.lin2.weight.reshape(-1))):\n",
    "    print(\"@\"+hex(i+len(model.lin1.weight.reshape(-1)))[2:].zfill(4)+\" \"+hex(struct.unpack('<I', struct.pack('<f', model.lin2.weight.reshape(-1)[i].item()))[0])[2:].zfill(8), end=\"|\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83ba95df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53462.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_model_params() * 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07924779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('fixed_image.hex','r') as file:\n",
    "    img = file.read().split(\"\\n\")\n",
    "hex_num = []\n",
    "for i in range(len(img)):\n",
    "    if img[i][6:] != '':\n",
    "        hex_num.append(img[i][6:])\n",
    "len(hex_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b994aa87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_num = []\n",
    "for i in range(len(hex_num)):\n",
    "    int_num.append(int(hex_num[i], 16))\n",
    "len(int_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "61b88d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -894.3608, -1501.9448,  3083.2507,  1188.5089,  -864.6537,  -882.4614,\n",
       "          -802.2708,  -210.8804,   569.8717,    87.4840]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    input = torch.FloatTensor(int_num).reshape(1, 784)\n",
    "    input = input.to(device)\n",
    "    output, _ = model(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a3e347f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3083.250732421875"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(output.reshape(10).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c8de491b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.reshape(10).tolist().index(max(output.reshape(10).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eed0840f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1fd3d89e9a0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize=(1, 1))\n",
    "cols, rows = 1, 1\n",
    "sample_idx = 5\n",
    "figure.add_subplot(rows, cols, 1)\n",
    "plt.title(label) \n",
    "plt.axis(\"off\")\n",
    "plt.imshow(torch.FloatTensor(int_num).view(28,28), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddea04f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
